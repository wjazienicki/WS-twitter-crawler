{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pymongo\n",
    "!python -m pip install pandas\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code's been found on stackoverflow at https://stackoverflow.com/a/16255680/10392851\n",
    "#it allows to connect to mongodatabase and read data from it to store in dataframe\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = pymongo.MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = pymongo.MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads data from mongodb into pandas dataframe\n",
    "\n",
    "#here, include names of database and collection of your designated MongoDB server\n",
    "database_name = \"<INPUT>\"\n",
    "collection_name = \"<INPUT>\"\n",
    "data = read_mongo(database_name,collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters out only tweets that are not retweets nor quotes\n",
    "\n",
    "data_only_selftweets = data[(data['retweeted_status'].isnull()) & (data['quoted_status'].isnull())]\n",
    "\n",
    "data_only_selftweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the next few steps I modify the data frame so that for each tweet data it holds the full text in one column\n",
    "#earlier, shorter tweets had its data in text, whereas longer ones in dict in \"extended_tweet\"\n",
    "\n",
    "data_only_selftweets_text = data_only_selftweets[['text', 'extended_tweet']]\n",
    "data_only_selftweets_text.extended_tweet = data_only_selftweets_text.extended_tweet.apply(lambda x: x[\"full_text\"] if (type(x) == dict) else x)\n",
    "\n",
    "#data_only_selftweets_text.fillna(data_only_selftweets_text['text']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only_selftweets_text['extended_tweet'] = data_only_selftweets_text['extended_tweet'].fillna(data_only_selftweets_text['text'])\n",
    "\n",
    "data_justtext = data_only_selftweets_text.drop(['text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the original dataframe with filtered out retweets and full text of tweets in one column which will be useful for clustering\n",
    "\n",
    "data_only_selftweets.text = data_justtext\n",
    "\n",
    "data_only_selftweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "# Verify they are empty.\n",
    "print(nlp.pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and normalize functions needed to improve clustering\n",
    "\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "def normalize(tokens):\n",
    "  normalized = list()\n",
    "  for token in tokens:\n",
    "    if (token.is_alpha or token.is_digit):\n",
    "      lemma = token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_\n",
    "      normalized.append(lemma)\n",
    "  return normalized\n",
    "\n",
    "def tokenize_normalize(string):\n",
    "    return normalize(spacy_tokenize(string))\n",
    "# A function that given an input query item returns the top-k most similar items \n",
    "# by their cosine similarity.\n",
    "def find_similar(query_vector, td_matrix, top_k = 5):\n",
    "    cosine_similarities = cosine_similarity(query_vector, td_matrix).flatten()\n",
    "    related_doc_indices = cosine_similarities.argsort()[::-1]\n",
    "    return [(index, cosine_similarities[index]) for index in related_doc_indices][0:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text field\n",
    "tweets_tokenized = data_only_selftweets.text.apply(spacy_tokenize)\n",
    "\n",
    "#print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_vals = data_only_selftweets.text\n",
    "ngram_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf=True, max_features=50000, ngram_range=(1,2))\n",
    "ngram_document_term_matrix = ngram_vectorizer.fit_transform(post_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section performs the clustering on text data from collected tweets\n",
    "\n",
    "document_matrix = ngram_document_term_matrix\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='random', n_init=5, verbose=10)\n",
    "kmeans.fit(document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, most common keywords in each grouping of tweets are printed\n",
    "\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = ngram_vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(' %s' % terms[ind])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
